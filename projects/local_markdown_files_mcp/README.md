# Markdown Files MCP

## How to run?

### Step 1.
- Activate ollama service

```
ollama serve
```

### Step 2.
Run the orchestrator script to verify that the LLM can successfully see the local files:

```bash
python3 chat_list_notes.py

OR

python3 chat_read_note.py
```
